---
title: "Diabetes Progression: Multi-Path Selection"
author: "Group 4"
output:
  rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Diabetes Progression: Multi-Path Selection}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  
GPT Link: https://chatgpt.com/share/6931b7b2-2cd8-8004-bbca-b83dbcf6313a
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
install.packages("remotes", repos = "https://cloud.r-project.org")
remotes::install_github("R-4-Data-Science/Final-Project-G4")
library(MultiPathSelection)
library(care)
library(dplyr)
set.seed(123)
```

## 1. Data

#### Using the Diabetes progression dataset from Efron et. al. (2004).

```{r}
data(efron2004)

# The efron2004$x has 'AsIs' class which causes issues with as.data.frame
# We need to remove this class and work with the raw matrix
x_matrix <- unclass(efron2004$x)
y <- efron2004$y

# Verify we have the column names
cat("Column names:", colnames(x_matrix), "\n")

# Create display dataframe
df <- as.data.frame(x_matrix)
df$y <- y

knitr::kable(
  head(df), 
  caption = "Clean Preview: Predictors + Outcome",
  digits = 2
)
```

#### Split outcome and predictors

```{r}
x <- as.data.frame(x_matrix)
# Verify column names are preserved
cat("Number of predictors:", ncol(x), "\n")
cat("Predictor variables:", paste(colnames(x), collapse = ", "), "\n")
```

## 2. Train/Test Split

```{r}
n <- nrow(x)
train_idx <- sample(n, floor(0.7 * n))
x_train <- x[train_idx, ]
y_train <- y[train_idx]
x_test  <- x[-train_idx, ]
y_test  <- y[-train_idx]
```

## 3. Multi-Path Search

#### We run the multi-path selection using:

- **K = 5**: Maximum number of steps allows the algorithm to explore models with up to 5 variables
- **eps = 0.01**: Requires very small AIC improvement to allow exploration of marginally informative variables  
- **delta = 4**: Keeps all competitive models within 4 AIC units of the best, allowing diverse model exploration
- **L = 15**: Limits computational cost by retaining only the 15 best models per step

```{r}
paths <- build_paths(
  x = x_train,
  y = y_train,
  family = "gaussian",
  K = 5,
  eps = 0.01,
  delta = 4,
  L = 15
)

# Summary table
summary_df <- data.frame(
  Metric = c("Steps Completed", "Total Models Explored", "Number of Variables"),
  Value = c(paths$meta$n_steps, paths$meta$total_models_explored, paths$meta$n_vars)
)
knitr::kable(summary_df, caption = "Multi-Path Search Summary")

# Debug: Show all frontiers
cat("\n### Debug: All Frontiers\n")
for (i in seq_along(paths$frontiers)) {
  cat("\nFrontier", i-1, ":\n")
  print(paths$frontiers[[i]])
}
```

## 4. Stability Selection (B = 50)

- **B = 50**: Uses 50 bootstrap resamples to assess how reliably each variable is selected across different data samples

```{r}
stab <- stability(
  x = x_train,
  y = y_train,
  B = 50,
  resample = "bootstrap",
  family = "gaussian",
  K = 5,
  eps = 0.01,
  delta = 4,
  L = 15
)
pi <- stab$pi

# Sort and display
pi_sorted <- sort(pi, decreasing = TRUE)
pi_df <- data.frame(
  Variable = names(pi_sorted),
  Stability = round(pi_sorted, 3)
)
knitr::kable(pi_df, caption = "Variable Stability Scores", row.names = FALSE)
```

## 5. Plausible Model Set

#### We filter models that:

- **Delta = 4**: Retains models within 4 AIC units of the best model, allowing consideration of multiple competitive alternatives
- **tau = 0.4**: Requires variables to be selected in at least 40% of bootstrap samples, balancing exploration with stability

```{r}
plausible <- plausible_models(
  path_forest = paths,
  pi = pi,
  Delta = 4,
  tau = 0.4
)

# Debug: Show what happened during filtering
cat("\n### Debug: Plausible Model Filtering\n")
cat("Total unique models before filtering:", length(unique(unlist(lapply(paths$frontiers, function(f) f$model_id)))), "\n")

if (nrow(plausible) > 0) {
  plausible_display <- plausible
  plausible_display$variables <- sapply(plausible$variables, paste, collapse = ", ")
  plausible_display$pi_bar <- round(plausible_display$pi_bar, 3)
  plausible_display$aic <- round(plausible_display$aic, 2)
  
  knitr::kable(plausible_display, caption = "Plausible Models", row.names = FALSE)
} else {
  print("No plausible models found.")
}
```

## 6. Fit the Best Plausible Model

#### Select variables of the top plausible model

```{r}
if (nrow(plausible) > 0) {
  best_vars <- plausible$variables[[1]]
} else {
  best_vars <- character(0)
}
print(best_vars)
```

#### Fit final model:

```{r}
# Get top plausible model variables
if (nrow(plausible) == 0 || length(plausible$variables[[1]]) == 0) {
  # fallback: intercept-only model
  final_model <- lm(y ~ 1, data = data.frame(y = y_train))
  message("No variables passed the plausible filter. Fitting intercept-only model.")
} else {
  best_vars <- plausible$variables[[1]]
  
  # build formula safely
  formula_str <- paste("y ~", paste(best_vars, collapse = " + "))
  final_model <- lm(as.formula(formula_str), data = data.frame(y = y_train, x_train))
}

# View summary
summary(final_model)
```

## 7. Test-Set Performance

#### Compute MSE on test data

```{r}
pred <- predict(final_model, newdata = data.frame(x_test))
mse_test <- mean((y_test - pred)^2)

perf_df <- data.frame(
  Metric = "Test MSE",
  Value = round(mse_test, 2)
)
knitr::kable(perf_df, caption = "Test Set Performance", row.names = FALSE)
```
